{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2425/blob/main/docs/labs/lab1/SPBD_Labs_mapreduce1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKGXsDUIDxH6"
      },
      "source": [
        "# Python MapReduce Example\n",
        "\n",
        "Word count implemented in pure Python.\n",
        "\n",
        "This notebook exemplifies the execution of a map-reduce program in Python, using Hadoop.\n",
        "In this example, hadoop runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBR4IYlXDxH9"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "B68ZyGdPDxH9"
      },
      "outputs": [],
      "source": [
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8sllMoIDxH_"
      },
      "source": [
        "## WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two steps:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQd9aMLeDxH_"
      },
      "source": [
        "### Mapper\n",
        "\n",
        "By starting an element with \"%%file\", you are specifying that when run, the contents are written to the local disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbItx5zwDxIA"
      },
      "outputs": [],
      "source": [
        "%%file mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys\n",
        "import sys\n",
        "# import string library function\n",
        "import string\n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # remove punctuation characters\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "    print('words\\t%s' % len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f1-nQlcDxIA"
      },
      "source": [
        "### Reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCDCm5cnDxIB"
      },
      "outputs": [],
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "total_count = 0\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    key, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    count = int(count)\n",
        "\n",
        "    total_count += count\n",
        "\n",
        "print('words\\t%s' % (total_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOA47uRDxIC"
      },
      "source": [
        "## Local execution\n",
        "\n",
        "The scripts can be tested using just the unix shell, as follows..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cORkS8pvDxIC"
      },
      "source": [
        "### Make the scripts executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuBS8D79DxID"
      },
      "outputs": [],
      "source": [
        "!chmod a+x mapper.py && chmod a+x reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dZTi7eIDxID"
      },
      "source": [
        "### Execute\n",
        "\n",
        "The execution workflow is as follows:\n",
        "\n",
        "+ The input file is piped into the input of the mapper;\n",
        "+ The output the mapper is sorted;\n",
        "+ The sorted output of the mapper is fed to the reducer stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwZMaB-hDxIE"
      },
      "outputs": [],
      "source": [
        "!cat \"os_maias.txt\" | ./mapper.py | sort -k1,1 | ./reducer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MapReduce with HADOOP"
      ],
      "metadata": {
        "id": "rwuFmaseENu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Hadoop on Google Colab\n",
        "!curl -s https://raw.githubusercontent.com/smduarte/spbd-2425/main/scripts/install_hadoop.sh | bash"
      ],
      "metadata": {
        "id": "GnLDSPxo4gKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC2bKZHtDxIE"
      },
      "source": [
        "## Hadoop standalone mode execution\n",
        "\n",
        "For executing in an hadoop cluster, input data should be moved into an HDFS directory. For executing in standalone mode, data can be read from the local filesystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z1JJmqvDxIE"
      },
      "source": [
        "\n",
        "The output directory needs to be cleared..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCwnGGz7DxIF"
      },
      "outputs": [],
      "source": [
        "rm -rf results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xf3e1UGDxIF"
      },
      "source": [
        "### Submitting the job\n",
        "\n",
        "The _hadoop_ command is used to submit the mapreduce job to the cluster..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTxBwSF8DxIG"
      },
      "outputs": [],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-*streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input os_maias.txt -output results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzJQj5qDxIG"
      },
      "source": [
        "#### Checking the results\n",
        "The result is stored in directory results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Tow7rp4DxIG"
      },
      "outputs": [],
      "source": [
        "!cat results/part-*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MrJob MapReduce Python Example\n",
        "\n",
        "Word count implemented in pure Python, using the library MrJob.\n",
        "\n",
        "[MrJob](https://mrjob.readthedocs.io/en/latest/) can be used to write MapReduce jobs and run them on several platforms.\n",
        "\n",
        "Some key advantages:\n",
        "+ Write **multi-step** MapReduce jobs in pure Python;\n",
        "+ Test on your **local machine**;\n",
        "+ Deploy jobs in several cloud plataforms of several vendors."
      ],
      "metadata": {
        "id": "z9ho--Lngp-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install MrJob\n",
        "!pip install mrjob --quiet\n",
        "!wget -q -O /etc/mrjob.conf https://raw.githubusercontent.com/smduarte/spbd-2425/main/scripts/mrjob.conf"
      ],
      "metadata": {
        "id": "udkLFwfBg2cU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MrJob WordCount Example\n",
        "Read the words from input and count them.\n",
        "\n",
        "The processing is split into two main phases:\n",
        "\n",
        "+ The mapper emits for each line the number of words\n",
        "+ The reduces sums all the tuples produced by the mapper stage...\n",
        "\n",
        "Using MrJob, a MapReduce job can be expressed in a single Python class,\n",
        "with methods for each of the phases. The reducer phase is called separately for each key, with the collection of values to be reduced."
      ],
      "metadata": {
        "id": "IFzQYN-QhdI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file wordcount.py\n",
        "\n",
        "import string\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCount(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "      # remove leading and trailing whitespace\n",
        "      line = line.strip()\n",
        "      # remove punctuation characters\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "      # split the line into words\n",
        "      yield \"words\", len(line.split())\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordCount.run()"
      ],
      "metadata": {
        "id": "g2FEWd44hQfY",
        "outputId": "5d641145-a84e-4c2b-c6bd-3b4747a4a735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wordcount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution of MrJob programs\n"
      ],
      "metadata": {
        "id": "mfowlYDChlCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Local Execution (Option 1)\n",
        "import importlib\n",
        "import wordcount\n",
        "importlib.reload(wordcount)\n",
        "\n",
        "# prepare the mapreduce job for local execution\n",
        "mr_job = wordcount.MRWordCount(args=['-r', 'local','os_maias.txt'])\n",
        "\n",
        "# execute the job and print the output results\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        print( key, value)"
      ],
      "metadata": {
        "id": "TPXIXoFjhsIU",
        "outputId": "5f8186b6-5b37-4517-8f2e-980148f27f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words 213359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Local Execution (Option 2)\n",
        "\n",
        "!python  wordcount.py -r local os_maias.txt"
      ],
      "metadata": {
        "id": "0YWjlaknhwJ-",
        "outputId": "3bd34293-3066-4ad7-a8db-6226121249ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using configs in /etc/mrjob.conf\n",
            "Creating temp directory /tmp/wordcount.root.20240917.151305.253333\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/wordcount.root.20240917.151305.253333/output\n",
            "Streaming final output from /tmp/wordcount.root.20240917.151305.253333/output...\n",
            "\"words\"\t213359\n",
            "Removing temp directory /tmp/wordcount.root.20240917.151305.253333...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supplying a combiner..."
      ],
      "metadata": {
        "id": "G7lJm6d7jDIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file wordcount_with_combiner.py\n",
        "\n",
        "import string\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCountWithCombiner(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "      # remove leading and trailing whitespace\n",
        "      line = line.strip()\n",
        "      # remove punctuation characters\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation+'«»'))\n",
        "      # split the line into words\n",
        "      yield \"words\", len(line.split())\n",
        "\n",
        "    def combiner(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordCountWithCombiner.run()"
      ],
      "metadata": {
        "id": "puc4AtH3jD9B",
        "outputId": "2015c1b0-e542-4a4e-e9c1-e06fa74a4c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wordcount_with_combiner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Local Execution\n",
        "\n",
        "import wordcount_with_combiner\n",
        "\n",
        "import importlib\n",
        "importlib.reload(wordcount_with_combiner)\n",
        "\n",
        "# prepare the mapreduce job for local execution\n",
        "mr_job = wordcount_with_combiner.MRWordCountWithCombiner(args=['-r', 'local','os_maias.txt'])\n",
        "\n",
        "# execute the job and print the output results\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        print( key, value)"
      ],
      "metadata": {
        "id": "L8bVz97sjLjX",
        "outputId": "f3e2b508-76e4-4d97-cd0a-e8aa94b4968b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words 213359\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}