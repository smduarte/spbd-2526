{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2425/blob/main/docs/labs/lab9/SPBD_Labs_spark4_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Streaming Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Example\n",
        "### Weblog Sender\n",
        "The stream server is a small python TCP server, listening\n",
        "on port 7777 (localhost).\n",
        "\n",
        "The stream will consist of a set of text lines, obtained from the output log of a webserver."
      ],
      "metadata": {
        "id": "eOg36Qo5-R15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python logsender/server.py logsender/web.log 7777 > /dev/null 2> /dev/null &"
      ],
      "metadata": {
        "id": "FVHbeNSi9_6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark UnStructured Streaming (Spark Core Streaming)\n",
        "\n",
        "The python code below shows the basics needed to process data from socket source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)"
      ],
      "metadata": {
        "id": "xg7YFVt1_I-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "import tempfile\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('WebLogExample').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  ssc = StreamingContext.getActiveOrCreate(checkpointPath = tempfile.TemporaryDirectory().name, setupFunc = lambda : StreamingContext(sc, 1))\n",
        "  lines = ssc.socketTextStream(\"localhost\", 7777)\n",
        "\n",
        "  lines.pprint()\n",
        "\n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(10)\n",
        "except Exception as err:\n",
        "  ssc.stop()"
      ],
      "metadata": {
        "id": "TkbJJN7h_IUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Exercises\n",
        "\n",
        "Do the follwing exercises:\n",
        "\n",
        "**Every 3 seconds**,\n",
        "1. Dump the number of requests in the last 10 seconds;\n",
        "2. Dump the number of requests in the last 10 seconds, only if they total more than 100;\n",
        "3. Dump the number of requests in the last 10 seconds, if there is an IP address with more than 100 requests;\n",
        "4. Dump the proportion of IPv4 vs IPv6 requests in the last 20 seconds.\n"
      ],
      "metadata": {
        "id": "h7BidJJE_lW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Spark Streaming\n",
        "\n",
        "The python code below shows the basics needed to process data from socket source using PySpark.\n",
        "\n",
        "Spark Structured Streaming (Spark SQL Streaming) python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html#)"
      ],
      "metadata": {
        "id": "izpS6n8W_5vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start the Structured Source\n",
        "\n",
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/json_logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python json_logsender/server.py json_logsender/web.log 8888 > /dev/null 2> /dev/null &"
      ],
      "metadata": {
        "id": "7qVr9MSlIlsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"timestamp\": \"2024-11-13T10:50:59.936+0000\", \"ip\": \"37.139.9.11\", \"code\": 404, \"cmd\": \"GET\", \"url\": \"/codemove/TTCENCUFMH3C\", \"time\": 0.026}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 8888) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "\n",
        "  query = json_lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  query.stop()\n"
      ],
      "metadata": {
        "id": "pH_2A7kOASSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Exercises\n",
        "\n",
        "Do the follwing exercises:\n",
        "\n",
        "**Every 3 seconds**,\n",
        "1. Dump the number of requests in the last 10 seconds;\n",
        "2. Dump the number of requests in the last 10 seconds, only if they total more than 100;\n",
        "3. Dump the number of requests in the last 10 seconds, if there is an IP address with more than 100 requests;\n",
        "4. Dump the proportion of IPv4 vs IPv6 requests in the last 20 seconds."
      ],
      "metadata": {
        "id": "eCV88xx9Dd1i"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}