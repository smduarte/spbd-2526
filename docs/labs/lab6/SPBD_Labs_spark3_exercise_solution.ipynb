{
  "cells": [
  {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/spbd-2425/blob/main/docs/labs/lab6/SPBD_Labs_spark3_exercise_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark SQL Exercises\n",
        "\n",
        "For this set of exercises, you should use SQL statements, as\n",
        "much as possible!\n",
        "\n",
        "Check this online resource for some help with [SQL queries](https://www.codecademy.com/learn/learn-sql/modules/learn-sql-queries/cheatsheet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pyspark\n",
        "!pip install --quiet pyspark"
      ],
      "metadata": {
        "id": "BuFS4wO2B1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!wc os_maias.txt"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Sorted Word Frequency\n",
        "\n",
        "1.1) Create a [Spark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) program that counts the number of occurrences of each word in \"Os Maias\" novel, sorting them by frequency (the words with higher occurrence first).\n"
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark Dataframes program that computes the top 10 most used words in \"Os Maias\" novel."
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "  .filter( lambda line : len(line) > 1 ) \\\n",
        "  .map( lambda line : Row( line = line ) )\n",
        "\n",
        "  linesDF = spark.createDataFrame( lines )\n",
        "  linesDF.createOrReplaceTempView(\"OSMAIAS\")\n",
        "\n",
        "  x = spark.sql(\"SELECT word, count(*) as freq FROM \\\n",
        "                  (SELECT explode(split(line, ' ')) as word FROM OSMAIAS) \\\n",
        "                  GROUP BY word ORDER BY freq DESC LIMIT 10\")\n",
        "\n",
        "  x.show(20)\n",
        "except Exception as err:\n",
        "  print(err)"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Weblog Analysis\n",
        "\n",
        "Consider a set of log files captured during a DDOS (*Distributed Denial of Service*) attack, containing information for the web accesses performed during the attack to the server.\n",
        "\n",
        "The log files contain text lines as shown below, with TAB as the separator:\n",
        "\n",
        "date |IP_source | status_code | operation | URL | execution time |\n",
        "-|-|-|-|-|-\n",
        "timestamp  | string | int | string | string| float |\n",
        "2016-12-06T08:58:35.318+0000|37.139.9.11|404|GET|/codemove/TTCENCUFMH3C|0.026"
      ],
      "metadata": {
        "id": "rsJZWYlHZDJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "!wget -q -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
        "!head -1 web.log\n",
        "\n",
        "!echo \"date ipSource retValue op url time\" > weblog_with_header.log\n",
        "!cat web.log >> weblog_with_header.log\n",
        "!head -2 weblog_with_header.log"
      ],
      "metadata": {
        "id": "WCWKj68qCOdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Count the number of unique IP addresses involved in the DDOS attack.\n"
      ],
      "metadata": {
        "id": "N1-ojIAqCftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    logRows = spark.read.csv('weblog_with_header.log',\n",
        "                             sep =' ', header=True, inferSchema=True)\n",
        "\n",
        "    logRows.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "#    x = spark.sql(\"SELECT count(*) FROM \\\n",
        "#                    (SELECT DISTINCT ipSource FROM WEBLOG)\")\n",
        "\n",
        "    x = spark.sql(\"SELECT count(DISTINCT ipSource) FROM WEBLOG\")\n",
        "\n",
        "    x.show()\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "Y7XoyNETChb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. For each interval of 10 seconds, provide the following information: [number of requests, average execution time, maximum time, minimum time]"
      ],
      "metadata": {
        "id": "ZJ5TzPdACgQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    logRows = spark.read.csv('weblog_with_header.log',\n",
        "                             sep =' ', header=True, inferSchema=True)\n",
        "\n",
        "    logRows.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "\n",
        "    x = spark.sql(\"SELECT from_unixtime((unix_timestamp(date) div 10) * 10) as intervalo, count(*) as requests, \\\n",
        "                      min(time), max(time), mean(time) FROM WEBLOG \\\n",
        "                      GROUP BY intervalo \\\n",
        "                      ORDER BY intervalo\")\n",
        "\n",
        "    x.show()\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "M8UVCwcdCwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Create an inverted index that, for each interval of 10 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "jUHmctaICgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    logRows = spark.read.csv('weblog_with_header.log',\n",
        "                             sep =' ', header=True, inferSchema=True)\n",
        "    logRows.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    spark.udf.register(\"toInterval\", lambda x : x[0:18])\n",
        "\n",
        "    x = spark.sql(\"SELECT from_unixtime((unix_timestamp(date) div 10) * 10) as intervalo, collect_set( ipSource) as Ips FROM WEBLOG \\\n",
        "                          GROUP BY intervalo \\\n",
        "                          ORDER BY intervalo DESC\")\n",
        "\n",
        "    x.show(truncate=False)\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "RpXghha0C0jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4. Create an inverted index that, for each interval of 15 seconds, has a list of (unique) IPs executing accesses (to each URL)."
      ],
      "metadata": {
        "id": "T6W21dG8R2MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('weblog').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    logRows = spark.read.csv('weblog_with_header.log',\n",
        "                             sep =' ', header=True, inferSchema=True)\n",
        "    logRows.createOrReplaceTempView(\"WEBLOG\")\n",
        "\n",
        "    x = spark.sql(\"SELECT from_unixtime((unix_timestamp(date) div 15) * 15) as intervalo, collect_set(ipSource) as Ips \\\n",
        "                      FROM WEBLOG GROUP BY intervalo ORDER BY intervalo\")\n",
        "\n",
        "    x.show(truncate = False)\n",
        "except Exception as err:\n",
        "    print(err)\n"
      ],
      "metadata": {
        "id": "f-OjejqGR8W9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
